from langchain_community.vectorstores import FAISS  # âœ… FAISSìš© ëª¨ë“ˆ
import zipfile
import os
import streamlit as st
import tempfile
import io
import atexit
import pandas as pd
import re
from google.cloud import vision
from dotenv import load_dotenv
import base64
import difflib
from langchain_openai import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableMap, RunnableLambda, RunnablePassthrough
import streamlit as st
from dotenv import load_dotenv
import os
import json
import tempfile
from google.oauth2 import service_account
from google.cloud import vision
import io


def show():
    
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    openai_key = st.secrets["OPENAI_API_KEY"]
    
    # google_credentialsë¥¼ dictë¡œ ë³€í™˜
    creds_info = dict(st.secrets["google_credentials"])
    
    # ì„ì‹œ JSON íŒŒì¼ ìƒì„±
    with tempfile.NamedTemporaryFile(mode="w+", suffix=".json", delete=False) as f:
        json.dump(creds_info, f)  # ğŸ”¥ ì—¬ê¸°ì„œ dictë§Œ ê°€ëŠ¥
        f.flush()
        credentials = service_account.Credentials.from_service_account_file(f.name)

    def get_image_base64(image_path):
        with open(image_path, "rb") as image_file:
            encoded = base64.b64encode(image_file.read()).decode()
        return f"data:image/png;base64,{encoded}"

    # ë§í’ì„  ìŠ¤íƒ€ì¼ í•¨ìˆ˜ (ì•„ì´ì½˜ ì¶”ê°€)
    def chat_message(role, message):
        if role == "user":
            st.markdown(
                f"""
                <div style="display: flex; justify-content: flex-end; align-items: flex-end; margin: 5px 0;">
                    <div style="background-color: #DCF8C6; padding: 10px 15px; 
                                border-radius: 15px 15px 0px 15px; max-width: 70%;
                                font-size: 16px; word-wrap: break-word;">
                        {message}
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )
        else:
            image_base64 = get_image_base64("ì œëª©.png")
            st.markdown(
                f"""
                <div style="display: flex; justify-content: flex-start; align-items: flex-start; margin: 5px 0;">
                    <img src="{image_base64}" width="45" height="45" style="margin-right: 8px; border-radius: 50%;">
                    <div style="background-color: #FFFFFF; padding: 10px 15px; 
                                border-radius: 15px 15px 15px 0px; max-width: 70%;
                                border: 1px solid #ddd; font-size: 16px; 
                                word-wrap: break-word;">
                        {message}
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )

    # Chroma DBë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì •ì˜
    def close_vectorstore():
        if "vectorstore" in st.session_state:
            print("âœ… FAISS vectorstoreëŠ” ë³„ë„ ì¢…ë£Œí•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.")


    # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ close_vectorstore í•¨ìˆ˜ê°€ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ë„ë¡ ë“±ë¡
    atexit.register(close_vectorstore)

    def detect_text(image_path):
        client = vision.ImageAnnotatorClient(credentials=credentials)
        with io.open(image_path, 'rb') as image_file:
            content = image_file.read()
    
        image = vision.Image(content=content)
        response = client.text_detection(image=image)
    
        if response.error.message:
            raise Exception(f"Google Vision API ì˜¤ë¥˜: {response.error.message}")
    
        texts = response.text_annotations
        if not texts:
            return ""
    
        return texts[0].description

    # ì§ˆë¬¸ ìœ í˜• ë¶„ì„ í•¨ìˆ˜
    def analyze_question_type(prompt):
        if "ì„±ë¶„" in prompt or "ë¶„ì„" in prompt:
            return "ì‹í’ˆí‘œì‹œê¸°ì¤€.pdf"
        elif "ì‹ì´ ë²”ìœ„" in prompt or "ì‹ì´ë²”ìœ„" in prompt or "ë¹„ê±´" in prompt:
            return "ì‹ì´ë²”ìœ„.pdf"
        elif "ì•ŒëŸ¬ì§€" in prompt or "ì•Œë ˆë¥´ê¸°" in prompt:
            return "ì•ŒëŸ¬ì§€.pdf"
        elif "í™˜ê²½ ì˜í–¥" in prompt or "í™˜ê²½ì˜í–¥" in prompt:
            return "AGRIBALYSE.csv"
        elif "ìˆ˜ìì›" in prompt or "ìˆ˜ìì› ì˜í–¥" in prompt:
            return [
                "ìˆ˜ìì›ë¬¸ì„œ.pdf"
            ]
        elif "ì¼ì¼ ì„­ì·¨ëŸ‰" in prompt or "ì¹¼ë¡œë¦¬" in prompt:
            return "ì¹¼ë¡œë¦¬.pdf"
        return None

    # ì‹í’ˆ í•˜ìœ„ ê·¸ë£¹
    FOOD_SUBGROUP_LIST = [
        "ì¡°ë¦¬ ë³´ì¡° ì‹í’ˆ", "í•´ì¡°ë¥˜", "ì¡°ë¯¸ë£Œ", "íŠ¹ë³„í•œ ì‹ì´ë¥¼ ìœ„í•œ ì‹í’ˆ", "í–¥ì‹ ë£Œ", "í—ˆë¸Œ", "ê¸°íƒ€ ì¬ë£Œ", 
        "ì†ŒìŠ¤", "ì†Œê¸ˆë¥˜", "ìœ ì•„ìš© ì‹œë¦¬ì–¼ ë° ë¹„ìŠ¤í‚·", "ìœ ì•„ìš© ì§  ì´ìœ ì‹ ë° ì‹ì‚¬ë¥˜", "í˜ì´ìŠ¤íŠ¸ë¦¬ì™€ ë‹¤ë¥¸ ì• í”¼íƒ€ì´ì €", 
        "í”¼ì, íƒ€ë¥´íŠ¸, ê·¸ë¦¬ê³  í¬ë ˆí˜", "í˜¼í•©ìš”ë¦¬", "ì±„ì‹ ìš”ë¦¬", "í˜¼í•© ìƒëŸ¬ë“œì™€ ìƒì•¼ì±„", "ìƒŒë“œìœ„ì¹˜", "ìˆ˜í”„", 
        "ê¸°íƒ€ ì§€ë°©ë¥˜", "ë²„í„°", "ì–´ìœ  (ìƒì„ ê¸°ë¦„)", "ì‹ë¬¼ì„± ê¸°ë¦„ ë° ì§€ë°©", "ë§ˆê°€ë¦°", "ì•„ì¹¨ìš© ì‹œë¦¬ì–¼ê³¼ ë¹„ìŠ¤í‚·", 
        "ë°€ê°€ë£¨ì™€ ë°˜ì£½", "íŒŒìŠ¤íƒ€, ìŒ€ ë° ê³¡ë¬¼", "ê¸°íƒ€ ìœ¡ë¥˜ ì œí’ˆ", "ê°€ê³µìœ¡", "ìƒë¬¼ê³¼ ê°‘ê°ë¥˜", "ê³„ë€", "ë‚  ìƒì„ ", 
        "ìµíŒ ìƒì„ ", "ìƒì„  ë° í•´ì‚°ë¬¼ ê°€ê³µ ì œí’ˆ", "ìœ¡ê°€ê³µí’ˆ ëŒ€ì²´í’ˆ", "ìœ¡ë¥˜ ëŒ€ì²´í’ˆ", "ìƒê³ ê¸°", "ìµíŒ ê³ ê¸°", 
        "í¬ë¦¼ ë° í¬ë¦¼ ê¸°ë°˜ íŠ¹ìˆ˜ ì‹í’ˆ", "ì¹˜ì¦ˆ", "ìš°ìœ ", "ì‹ ì„ í•œ ìœ ì œí’ˆ ë° ìœ ì‚¬ ì œí’ˆ", "ê³¼ì¼", "ê²¬ê³¼ë¥˜ ë° ìœ ì§€ì¢…ì", 
        "ì±„ì†Œ", "ì½©ë¥˜", "ê°ì ë° ê¸°íƒ€ ë¿Œë¦¬ì±„ì†Œ"
    ]

    # í™˜ê²½ ì˜í–¥ ë²”ì£¼ë³„ ê´€ë ¨ column ì •ì˜
    impact_categories = {
        "ìƒë¬¼í•™ì  ì˜í–¥": [
            "ì¸ê°„ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ë…ì„± ì˜í–¥: ë¹„ì•”ì„± ë¬¼ì§ˆ(ì œí’ˆ ë‹¹ CTUh/kg)",
            "ì¸ê°„ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ë…ì„± ì˜í–¥: ë°œì•” ë¬¼ì§ˆ(ì œí’ˆ ë‹¹ CTUh/kg)",
        ],
        "ëŒ€ê¸° ì˜í–¥": [
            "ê¸°í›„ ë³€í™”(ì œí’ˆ ë‹¹ CO2 eq/kg)",
            "ì˜¤ì¡´ì¸µ íŒŒê´´(ì œí’ˆ ë‹¹ CVC11 eq/kg)",
            "ì´ì˜¨í™” ë°©ì‚¬ì„ (ì œí’ˆ ë‹¹ U-235 eq/kg)",
            "ì˜¤ì¡´ì˜ ê´‘í™”í•™ì  í˜•ì„±(ì œí’ˆ ë‹¹ NMVOC eq/kg)",
            "ë¯¸ì„¸ ë¨¼ì§€(ì œí’ˆ ë‹¹ ì§ˆë³‘ ë°œìƒë¥ /kg)",
        ],
        "í† ì§€ ì˜í–¥": [
            "ìœ¡ìƒ ë° ë‹´ìˆ˜ ì‚°ì„±í™”(ì œí’ˆ ë‹¹ H+ eq/kg)",
            "ìœ¡ìƒ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ N eq/mol)",
            "í† ì§€ ì‚¬ìš©(ì œí’ˆ ë‹¹ Pt/kg)",
            "í™”ì„ ìì› ê³ ê°ˆ(ì œí’ˆ ë‹¹ MJ/kg)",
            "ê´‘ë¬¼ ìì› ê³ ê°ˆ(ì œí’ˆ ë‹¹ Sb eq/kg)",
        ],
        "ìˆ˜ìì› ì˜í–¥" : [
            "ë‹´ìˆ˜ì˜ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ P eq/kg)",
            "í•´ì–‘ì˜ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ N eq/kg)",
            "ë‹´ìˆ˜ ìˆ˜ìƒ ìƒíƒœê³„ì— ëŒ€í•œ ìƒíƒœ ë…ì„±(ì œí’ˆ ë‹¹ CTUe/kg)",
        ]
    }

    impact_factors = {
        "ìœ¡ìƒ ë° ë‹´ìˆ˜ ì‚°ì„±í™”(ì œí’ˆ ë‹¹ H+ eq/kg)": {"nf": 55.56954123, "weight": 6.2},
        "ê¸°í›„ ë³€í™”(ì œí’ˆ ë‹¹ CO2 eq/kg)": {"nf": 7553.083163, "weight": 21.06},
        "ë‹´ìˆ˜ ìˆ˜ìƒ ìƒíƒœê³„ì— ëŒ€í•œ ìƒíƒœ ë…ì„±(ì œí’ˆ ë‹¹ CTUe/kg)": {"nf": 56716.58634, "weight": 1.92},
        "ë‹´ìˆ˜ì˜ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ P eq/kg)": {"nf": 1.606852128, "weight": 2.8},
        "í•´ì–‘ì˜ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ N eq/kg)": {"nf": 19.54518155, "weight": 2.96},
        "ìœ¡ìƒ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ N eq/mol)": {"nf": 176.7549998, "weight": 3.71},
        "ì¸ê°„ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ë…ì„± ì˜í–¥: ë°œì•” ë¬¼ì§ˆ(ì œí’ˆ ë‹¹ CTUh/kg)": {"nf": 1.73E-05, "weight": 2.13},
        "ì¸ê°„ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ë…ì„± ì˜í–¥: ë¹„ì•”ì„± ë¬¼ì§ˆ(ì œí’ˆ ë‹¹ CTUh/kg)": {"nf": 0.000129, "weight": 1.84},
        "ì´ì˜¨í™” ë°©ì‚¬ì„ (ì œí’ˆ ë‹¹ U-235 eq/kg)": {"nf": 4220.16339, "weight": 5.01},
        "í† ì§€ ì‚¬ìš©(ì œí’ˆ ë‹¹ Pt/kg)": {"nf": 819498.1829, "weight": 7.94},
        "ì˜¤ì¡´ì¸µ íŒŒê´´(ì œí’ˆ ë‹¹ CVC11 eq/kg)": {"nf": 0.052348383, "weight": 6.31},
        "ë¯¸ì„¸ ë¨¼ì§€(ì œí’ˆ ë‹¹ ì§ˆë³‘ ë°œìƒë¥ /kg)": {"nf": 0.000595, "weight": 8.96},
        "ì˜¤ì¡´ì˜ ê´‘í™”í•™ì  í˜•ì„±(ì œí’ˆ ë‹¹ NMVOC eq/kg)": {"nf": 40.85919773, "weight": 4.78},
        "í™”ì„ ìì› ê³ ê°ˆ(ì œí’ˆ ë‹¹ MJ/kg)": {"nf": 65004.25966, "weight": 8.32},
        "ê´‘ë¬¼ ìì› ê³ ê°ˆ(ì œí’ˆ ë‹¹ Sb eq/kg)": {"nf": 0.063622615, "weight": 7.55},
        "ìˆ˜ìì› ê³ ê°ˆ(ì œí’ˆ ë‹¹ m3 depriv./kg)": {"nf": 11468.70864, "weight": 8.51}
    }

    # ì‚¬ìš©ì ì…ë ¥(prompt_text)ì—ì„œ ì‹í’ˆ í•˜ìœ„ ê·¸ë£¹(FOOD_SUBGROUP_LIST)ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    def match_food_subgroup_from_prompt(prompt_text):
        # FOOD_SUBGROUP_LISTì— ìˆëŠ” ê° ì„œë¸Œê·¸ë£¹ì„ ìˆœíšŒ
        for subgroup in FOOD_SUBGROUP_LIST:
            # ì…ë ¥ í…ìŠ¤íŠ¸ì— í•´ë‹¹ ì„œë¸Œê·¸ë£¹ ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  í™•ì¸
            if subgroup.lower() in prompt_text.lower():
                return subgroup  # ì¼ì¹˜í•˜ëŠ” ì„œë¸Œê·¸ë£¹ì´ ìˆìœ¼ë©´ ë°˜í™˜
        return None  # ì¼ì¹˜í•˜ëŠ” í•­ëª©ì´ ì—†ìœ¼ë©´ None ë°˜í™˜

    # ì˜í–¥ ì¹´í…Œê³ ë¦¬ë³„ ì—´ ì¶”ì¶œ í•¨ìˆ˜
    def extract_impact_columns(prompt):
        # í™˜ê²½ ì˜í–¥ ì¹´í…Œê³ ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í™•ì¸í•˜ì—¬ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì—´ì„ ë°˜í™˜
        for category, columns in impact_categories.items():  # impact_categories ë”•ì…”ë„ˆë¦¬ ìˆœíšŒ
            if category in prompt:  # í˜„ì¬ ì¹´í…Œê³ ë¦¬ëª…ì´ promptì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
                return columns  # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ëŒ€ì‘í•˜ëŠ” ì—´ ëª©ë¡ ë°˜í™˜
        return []  # ì¼ì¹˜í•˜ëŠ” ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì˜¤ë¥˜ ë°©ì§€ìš© ê¸°ë³¸ê°’)


    def calculate_environmental_impact(prompt, ocr_text):
        # ë²¡í„° DBì—ì„œ "AGRIBALYSE.csv" ë¬¸ì„œë§Œ ê²€ìƒ‰
        vectorstore = st.session_state["vectorstore"]

        # 'í™˜ê²½ ì˜í–¥' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì§ˆë¬¸ë§Œ ì²˜ë¦¬
        document_name = analyze_question_type(prompt)

        print(f"ë¬¸ì„œ ì´ë¦„: {document_name}")

        if not document_name:
            no_match_response = (
                "â—ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
                "ë‹¤ìŒê³¼ ê°™ì€ ì£¼ì œë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”:\n"
                "- ì„±ë¶„ ë¶„ì„, ë¹„ê±´ ì¢…ë¥˜, ì•Œë ˆë¥´ê¸°, í™˜ê²½ ì˜í–¥, ìˆ˜ìì›, ì¹¼ë¡œë¦¬ ë“±"
            )
            chat_message("assistant", no_match_response)
            st.session_state["memory"].chat_memory.add_ai_message(no_match_response)
            st.session_state.messages.append({"role": "assistant", "content": no_match_response})
            return None

        # FAISSì—ì„œëŠ” search_kwargsì— filter ì‚¬ìš© ë¶ˆê°€ â†’ ê²€ìƒ‰ í›„ ìˆ˜ë™ í•„í„°ë§
        retriever = vectorstore.as_retriever()
        relevant_docs = retriever.get_relevant_documents(prompt)

        # ë¬¸ì„œ ì´ë¦„ ê¸°ë°˜ í•„í„°ë§ (ex: 'AGRIBALYSE.csv'ë§Œ ì„ íƒ)
        if isinstance(document_name, str):
            relevant_docs = [doc for doc in relevant_docs if doc.metadata.get("source") == document_name]
        elif isinstance(document_name, list):
            relevant_docs = [doc for doc in relevant_docs if doc.metadata.get("source") in document_name]

        print(f"ê´€ë ¨ ë¬¸ì„œ: {[doc.metadata.get('product_name') for doc in relevant_docs]}")

        # ì‹í’ˆêµ° í•„í„°ë§
        subgroup = match_food_subgroup_from_prompt(prompt)
        if subgroup:
            relevant_docs = [doc for doc in relevant_docs if doc.metadata.get('food_subgroup') == subgroup]

        print(f"ì‹í’ˆêµ° í•„í„°ë§ëœ ë¬¸ì„œ: {[doc.metadata.get('product_name') for doc in relevant_docs]}")

        # OCR í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ë„ ë¹„êµ
        matching_docs = []
        for doc in relevant_docs:
            product_name = doc.metadata.get("product_name", "")
            similarity = difflib.SequenceMatcher(None, ocr_text, product_name).ratio()
            matching_docs.append((similarity, doc))

        print(f"ìœ ì‚¬ë„ ê³„ì‚°ëœ ë¬¸ì„œ: {[(doc[1].metadata.get('product_name'), doc[0]) for doc in matching_docs]}")

        matching_docs.sort(reverse=True, key=lambda x: x[0])
        if not matching_docs:
            print("ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        most_similar_doc = matching_docs[0][1]
        print(f"ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ: {most_similar_doc.metadata.get('product_name')}")

        # í™˜ê²½ ì˜í–¥ í•­ëª© ì¶”ì¶œ
        selected_cols = extract_impact_columns(prompt)
        print(f"ì„ íƒëœ í™˜ê²½ ì˜í–¥ í•­ëª©: {selected_cols}")

        impact_data = []

        if most_similar_doc:
            metadata = most_similar_doc.metadata
            impact_entry = {
                'food_subgroup': metadata.get('food_subgroup'),
                'product_name': metadata.get('product_name')
            }

            for col in selected_cols:
                impact_entry[col] = metadata.get(col, None)

            impact_data.append(impact_entry)

        print(f"í™˜ê²½ ì˜í–¥ ë°ì´í„°: {impact_data}")

        if impact_data:
            impact_df = pd.DataFrame(impact_data)
            print(f"ê²°ê³¼ DataFrame: {impact_df}")
            return impact_df
        else:
            print("í™˜ê²½ ì˜í–¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
    def extract_values_from_prompt(prompt):
        print(f"ì…ë ¥ëœ prompt: {prompt}")
        items_values = re.findall(r"([^\-]+) - ([0-9.E-]+)", prompt)
        print(f"ì¶”ì¶œëœ í•­ëª©ê³¼ ê°’: {items_values}")
        return items_values

    def get_user_inputs(prompt):
        print(f"ì²˜ë¦¬í•  prompt: {prompt}")
        items_values = extract_values_from_prompt(prompt)
        user_inputs = {}

        for item, value in items_values:
            item = item.strip()
            value = float(value)
            user_inputs[item] = value
            print(f"ì…ë ¥ ê°’ ì €ì¥: {item} -> {value}")

        return user_inputs

    def calculate_score(user_inputs):
        score = 0
        for factor, value in user_inputs.items():
            if factor in impact_factors:
                weight = impact_factors[factor]["weight"]
                nf = impact_factors[factor]["nf"]
                if nf != 0:
                    factor_score = (value / nf) * weight * 1000
                    score += factor_score
        return score

    # í™˜ê²½ ì˜í–¥ ê³„ì‚° í›„ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
    def calculate_environmental_impact_with_score(prompt):
        print(f"ì ìˆ˜ ê³„ì‚° ìš”ì²­: {prompt}")  # ë””ë²„ê¹…ìš© print
        if "ì ìˆ˜" in prompt or "í™˜ê²½ ì ìˆ˜" in prompt or "í™˜ê²½ ì˜í–¥ ì ìˆ˜" in prompt:  # ì ìˆ˜ ê´€ë ¨ ì§ˆë¬¸ë§Œ ì²˜ë¦¬
            user_inputs = get_user_inputs(prompt)  # ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ê°’ì„ ë°›ìŒ

            if user_inputs:
                # ì ìˆ˜ ê³„ì‚°
                final_score = calculate_score(user_inputs)
                # ì ìˆ˜ ê²°ê³¼ ë°˜í™˜
                score_response = f"ê³„ì‚°ëœ í™˜ê²½ ì˜í–¥ ì ìˆ˜: {final_score:.6f} mPt"
                print(f"ê²°ê³¼ ë°˜í™˜: {score_response}")  # ë””ë²„ê¹…ìš© print
                return score_response
            else:
                return "ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ì…ë ¥ ê°’ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
        return None

    def extract_gram_from_prompt(prompt):
        # ì‚¬ìš©ì ì…ë ¥ì—ì„œ g ë‹¨ìœ„ì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” ìˆ«ìë¥¼ ì¶”ì¶œ (ì˜ˆ: "600g", "150g")
        match = re.search(r'(\d+)\s*g\s*ë‹¹', prompt)
        return int(match.group(1)) if match else None

    def store_score_from_response(response, expected_gram=None):
        if "calorie_scores" not in st.session_state:
            st.session_state["calorie_scores"] = []

        if expected_gram is None:
            print("[âŒ ë¬´ì‹œë¨] g ë‹¨ìœ„ê°€ ì§ˆë¬¸ì— ì—†ìŒ.")
            return

        # ì˜ˆ: '600gë‹¹ 173kcal' ì™€ ì •í™•íˆ ë§¤ì¹­
        pattern = fr'{expected_gram}\s*g\s*ë‹¹.*?(\d+(?:\.\d+)?)\s*(?:kcal|ì¹¼ë¡œë¦¬)'  # 'gë‹¹ ì¹¼ë¡œë¦¬' ì¶”ì¶œ
        match = re.search(pattern, response)
        if match:
            score = float(match.group(1))
            st.session_state["calorie_scores"].append(score)
            print(f"[âœ… ì €ì¥ë¨] {expected_gram}gë‹¹ {score} kcal")
            print(f"[ğŸ“¦ ì „ì²´ ëª©ë¡] {st.session_state['calorie_scores']}")
        else:
            print(f"[âŒ ì €ì¥ ì‹¤íŒ¨] ì‘ë‹µì— {expected_gram}gë‹¹ ì¹¼ë¡œë¦¬ ì •ë³´ê°€ ì—†ìŒ.")


# Streamlit UI ì‹œì‘----------------------------------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# --- ì‚¬ì´ë“œë°” ---
    with st.sidebar:
        st.markdown("### ì‚¬ìš©ì ì •ë³´")
        user_info = st.session_state.get("user_info", {})
        st.markdown(f"**ì´ë¦„:** {user_info.get('name', 'ë¯¸ì…ë ¥')}")
        st.markdown(f"**ë¹„ê±´ ì¢…ë¥˜:** {', '.join(user_info.get('types', [])) if user_info.get('types') else 'ë¯¸ì…ë ¥'}")
        st.markdown(f"**ë‚˜ì´:** {user_info.get('age', 'ë¯¸ì…ë ¥')}")
        st.markdown(f"**ì„±ë³„:** {user_info.get('gender', 'ë¯¸ì…ë ¥')}")
        st.markdown(f"**ì•ŒëŸ¬ì§€:** {user_info.get('allergy', 'ì—†ìŒ')}")

        if st.button("ì •ë³´ ìˆ˜ì •"):
            st.session_state.page = "info"
            st.session_state.from_chatbot = False
            st.rerun()

        uploaded_image = st.file_uploader("ì‹í’ˆ ë¼ë²¨ ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["png", "jpg", "jpeg"])

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "memory" not in st.session_state:
        st.session_state["memory"] = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        st.session_state["ocr_text"] = None
    if "calorie_answers" not in st.session_state:
        st.session_state["calorie_answers"] = []
    
    # ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° OCR ì²˜ë¦¬
    if uploaded_image is not None:
        if "prev_uploaded_filename" not in st.session_state or st.session_state["prev_uploaded_filename"] != uploaded_image.name:
            st.session_state["prev_uploaded_filename"] = uploaded_image.name
    
            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                tmp_file.write(uploaded_image.getvalue())
                tmp_path = tmp_file.name
    
            try:
                ocr_text = detect_text(tmp_path)
                st.session_state["ocr_text"] = ocr_text
                st.session_state["ocr_done"] = True
                st.success("âœ… OCR ì²˜ë¦¬ ì™„ë£Œ! ì¶”ì¶œëœ í…ìŠ¤íŠ¸:")
                st.text_area("OCR í…ìŠ¤íŠ¸", ocr_text, height=300)
    
                system_message = f"ì•„ë˜ëŠ” ì‹í’ˆ ë¼ë²¨ OCR í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤:\n{ocr_text}"
                st.session_state["memory"].chat_memory.add_user_message(system_message)
    
            except Exception as e:
                st.error(f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.stop()
    
# âœ… OCR ì„±ê³µ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ í•­ìƒ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
if "vectorstore" not in st.session_state:
    zip_path = "/mount/src/-veganismchatbot/faiss_db_merged.zip"
    persist_dir = "/mount/src/-veganismchatbot/faiss_db_merged"

    if not os.path.exists(persist_dir):
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(persist_dir)
            print(f"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ: {persist_dir}")

    inner = os.listdir(persist_dir)
    if len(inner) == 1 and os.path.isdir(os.path.join(persist_dir, inner[0])):
        persist_dir = os.path.join(persist_dir, inner[0])
        print(f"ğŸ“‚ ì´ì¤‘ êµ¬ì¡° ê°ì§€ â†’ ë‚´ë¶€ ê²½ë¡œë¡œ ì´ë™: {persist_dir}")
        
    embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")
    st.session_state["vectorstore"] = FAISS.load_local(
        persist_dir,
        embedding_function,
        allow_dangerous_deserialization=True
    )
    print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ ì¶”ê°€ ì—†ìŒ)")

    

    # ì´ˆê¸° ë©”ì‹œì§€ ì¶œë ¥
    if "messages" not in st.session_state:
        st.session_state.messages = []
        chat_message("assistant", "ì—ì½” ë¹„ê±´ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

    for msg in st.session_state.messages:
        chat_message(msg["role"], msg["content"])

    prompt = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    if prompt:
        user_info_text = (
            f"ì‚¬ìš©ì ì •ë³´:\n"
            f"- ì´ë¦„: {user_info.get('name', '')}\n"
            f"- ë¹„ê±´ ì¢…ë¥˜: {', '.join(user_info.get('types', []))}\n"
            f"- ë‚˜ì´: {user_info.get('age', '')}\n"
            f"- ì„±ë³„: {user_info.get('gender', '')}\n"
            f"- ì•ŒëŸ¬ì§€: {user_info.get('allergy', '')}\n\n"
        )

        st.session_state.messages.append({"role": "user", "content": prompt})
        chat_message("user", prompt)


        # âœ… ì¢…ë£Œ ì¸ì‚¬ ê°ì§€ ë° ì‘ë‹µ â†’ ì¦‰ì‹œ return
        farewell_phrases = ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "ì§ˆë¬¸"]
        if any(phrase in prompt.lower() for phrase in farewell_phrases):
            farewell_response = "ì•ˆë…•í•˜ì„¸ìš”. ë¹„ê±°ë‹ˆì¦˜ ì±—ë´‡ì…ë‹ˆë‹¤. ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ë©´ ë­ë“  ë¬¼ì–´ë´ì£¼ì„¸ìš”!"
            chat_message("assistant", farewell_response)
            st.session_state["memory"].chat_memory.add_ai_message(farewell_response)
            st.session_state.messages.append({"role": "assistant", "content": farewell_response})
            return 


        if any(p in prompt for p in ["ì´í•©", "í•˜ë£¨", "ëª¨ë“  ì ìˆ˜"]):
            total_score = sum(st.session_state.get("calorie_scores", []))
            if total_score > 0:
                response = f"ì§€ê¸ˆê¹Œì§€ ì €ì¥ëœ ì¹¼ë¡œë¦¬ ì´í•©ì€ ì•½ {int(total_score)} kcal ì…ë‹ˆë‹¤."
            else:
                response = "ì•„ì§ ì¹¼ë¡œë¦¬ ì •ë³´ë¥¼ ì €ì¥í•œ ì ì´ ì—†ì–´ìš”. ğŸ¤”"
            chat_message("assistant", response)
            st.session_state["memory"].chat_memory.add_ai_message(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.stop()

# ìˆ˜ì • ì¤„ ì‹œì‘        
        keywords = ["ìˆ˜ìì›", "ì„±ë¶„", "ë¹„ê±´", "ì•ŒëŸ¬ì§€", "í™˜ê²½ ì˜í–¥", "ìˆ˜ìì›", "ì¹¼ë¡œë¦¬", "ì‹ì´ë²”ìœ„", "ê°ìì¹©", "í™˜ê²½ ì ìˆ˜", "í™˜ê²½ ì˜í–¥ ì ìˆ˜"]
# ìˆ˜ì • ì¤„  ë


        # 'ì ìˆ˜' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì§ˆë¬¸ì´ë©´ ì ìˆ˜ ê³„ì‚°ë§Œ ì¶œë ¥
        if "ì ìˆ˜" in prompt or "í™˜ê²½ ì ìˆ˜" in prompt or "í™˜ê²½ ì˜í–¥ ì ìˆ˜" in prompt:
            result = calculate_environmental_impact_with_score(prompt)
            
            if result:  # ì ìˆ˜ ê³„ì‚° ê²°ê³¼ê°€ ì¡´ì¬í•˜ë©´
                chat_message("assistant", result)  # ê³„ì‚°ëœ ì ìˆ˜ ì¶œë ¥
                st.session_state["memory"].chat_memory.add_ai_message(result)  # ê³„ì‚°ëœ ê²°ê³¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": result})  # ì„¸ì…˜ ë©”ì‹œì§€ì— ì¶”ê°€
            else:
                # ê³„ì‚° ê°’ì´ ì—†ì„ ê²½ìš°
                error_message = "â— ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ ê°’ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                chat_message("assistant", error_message)  # ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
                st.session_state["memory"].chat_memory.add_ai_message(error_message)  # ì˜¤ë¥˜ ë©”ì‹œì§€ ë©”ëª¨ë¦¬ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": error_message})  # ì˜¤ë¥˜ ë©”ì‹œì§€ ì„¸ì…˜ì— ì¶”ê°€
            return

        # ë‹¤ë¥¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì§ˆë¬¸ì€ ë¹„ê±´ ê´€ë ¨ ë©”ì‹œì§€ ì¶œë ¥
        if not any(keyword in prompt for keyword in keywords):
            no_relevant_msg = (
                "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì€ ë¹„ê±°ë‹ˆì¦˜ì— ê´€ë ¨ëœ ì§ˆë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤.ğŸ˜…ğŸ˜… "
                "ë¹„ê±´ë‹ˆì¦˜ì— ê´€ë ¨ëœ ì§ˆë¬¸ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"
            )
            chat_message("assistant", no_relevant_msg)  # ë‹µë³€ ì¶œë ¥
            st.session_state["memory"].chat_memory.add_ai_message(no_relevant_msg)  # ë©”ëª¨ë¦¬ì— ì €ì¥
            st.session_state.messages.append({"role": "assistant", "content": no_relevant_msg})  # ì„¸ì…˜ ë©”ì‹œì§€ì— ì¶”ê°€
            return
        
        # âœ… í™˜ê²½ ì˜í–¥ ì§ˆë¬¸ì´ í¬í•¨ëœ ê²½ìš°ì—ë§Œ í™˜ê²½ ì˜í–¥ ê³„ì‚° í•¨ìˆ˜ ì‹¤í–‰
        if "í™˜ê²½ ì˜í–¥" in prompt or "í™˜ê²½ì˜í–¥" in prompt:  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ promptì—ì„œ 'í™˜ê²½ ì˜í–¥' ë˜ëŠ” 'í™˜ê²½ì˜í–¥'ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
            # í™˜ê²½ ì˜í–¥ ê³„ì‚° í•¨ìˆ˜ í˜¸ì¶œ
            impact_df = calculate_environmental_impact(prompt, st.session_state["ocr_text"])  # 'calculate_environmental_impact' í•¨ìˆ˜ í˜¸ì¶œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ OCR í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•˜ì—¬ í™˜ê²½ ì˜í–¥ ë°ì´í„° ê³„ì‚°
            
            if impact_df is not None:  # í™˜ê²½ ì˜í–¥ ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ë°˜í™˜ë˜ì—ˆìœ¼ë©´
                # impact_dfë¥¼ HTML í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥ í˜•ì‹ ì§€ì •
                impact_response = f"<h3>í™˜ê²½ ì˜í–¥ ë°ì´í„°:</h3>{impact_df.to_html(index=False)}"
                chat_message("assistant", impact_response)  # assistant ì—­í• ë¡œ ìƒì„±ëœ í™˜ê²½ ì˜í–¥ ë°ì´í„°ë¥¼ ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ë¡œ ì¶œë ¥
                st.session_state["memory"].chat_memory.add_ai_message(impact_response)  # ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": impact_response})  # ë©”ì‹œì§€ë¥¼ 'assistant' ì—­í• ë¡œ ì„¸ì…˜ ë©”ì‹œì§€ì— ì¶”ê°€

            else:  # ë§Œì•½ í™˜ê²½ ì˜í–¥ ë°ì´í„°ê°€ None(ë¹ˆ ê°’)ì´ë¼ë©´
                chat_message("assistant", "â—ì£„ì†¡í•©ë‹ˆë‹¤. í™˜ê²½ ì˜í–¥ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")  # ë°ì´í„°ê°€ ì—†ë‹¤ëŠ” ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ì¶œë ¥
            return  # í•¨ìˆ˜ ì‹¤í–‰ì„ ì¢…ë£Œí•˜ê³ , ë” ì´ìƒ ë‹¤ë¥¸ ì½”ë“œê°€ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ í•¨
        
        if any(phrase in prompt for phrase in ["ì´í•©", "ì˜¤ëŠ˜", "í•˜ë£¨", "ì´ ì ìˆ˜", "ì „ì²´ ì ìˆ˜", "ëª¨ë“  ì ìˆ˜"]):
            total_score = sum(st.session_state.get("calorie_scores", []))  # âœ… ìˆ˜ì •ëœ ë¶€ë¶„
            if total_score > 0:
                response = f"ì§€ê¸ˆê¹Œì§€ ì €ì¥ëœ ì¹¼ë¡œë¦¬ ì´í•©ì€ ì•½ {int(total_score)} kcal ì…ë‹ˆë‹¤."
            else:
                response = "ì•„ì§ ì¹¼ë¡œë¦¬ ì •ë³´ë¥¼ ì €ì¥í•œ ì ì´ ì—†ì–´ìš”. ğŸ¤” '(150g ë‹¹ ì¹¼ë¡œë¦¬ëŠ” ëª‡ì´ì•¼?)'ì²˜ëŸ¼ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
            
            chat_message("assistant", response)
            st.session_state["memory"].chat_memory.add_ai_message(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

            # í•„ìš” ì‹œ: ì´ ì‘ë‹µì€ ì €ì¥ ì•ˆ í•´ë„ ë¨ (ì´í•© ê²°ê³¼ì´ë¯€ë¡œ)
            return


        if "vectorstore" not in st.session_state or st.session_state["ocr_text"] is None:
            chat_message("assistant", "â— ë¨¼ì € ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° OCR ì²˜ë¦¬ë¥¼ ì™„ë£Œí•´ ì£¼ì„¸ìš”.")
            st.stop()

        st.session_state["memory"].chat_memory.add_user_message(prompt)

        llm = ChatOpenAI(temperature=1)
        retriever = st.session_state["vectorstore"].as_retriever()

        document_name = analyze_question_type(prompt)
        if not isinstance(document_name, list):
            document_name = [document_name] if document_name else []
        document_name.append("user_ocr")  # OCR ë¬¸ì„œ í•­ìƒ í¬í•¨

        # ğŸ” FAISS: ê²€ìƒ‰ í›„ ìˆ˜ë™ í•„í„°ë§
        retrieved_docs = retriever.get_relevant_documents(prompt)
        filtered_docs = [doc for doc in retrieved_docs if doc.metadata.get("source") in document_name]

        # âœ… RAG í”„ë¡¬í”„íŠ¸ ì •ì˜ (1ë²ˆë§Œ)
        rag_prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ë¹„ê±°ë‹ˆì¦˜ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
        - ì‚¬ìš©ìì˜ ë¹„ê±´ ì¢…ë¥˜ì™€ ì•ŒëŸ¬ì§€ë¥¼ ê³ ë ¤í•´ ìµœëŒ€í•œ ì •í™•íˆ ë‹µí•´ì£¼ì„¸ìš”.

        ì‚¬ìš©ì ì§ˆë¬¸: {question}

        OCR í…ìŠ¤íŠ¸: {ocr_text}

        ì°¸ê³  ë¬¸ì„œ: {context_docs}
        """)


        # âœ… RagChain ì •ì˜ - FAISS í˜¸í™˜ ë²„ì „
        rag_chain = (
            RunnableMap({
                "question": RunnablePassthrough(),
                "ocr_text": RunnablePassthrough(),
                "context_docs": lambda input: "\n\n".join(doc.page_content for doc in filtered_docs),
            })
            | rag_prompt
            | llm
            | StrOutputParser()
        )

        # âœ… OCR ë° ë¹„ê±´ ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œë§Œ ì‚¬ìš©ì ì •ë³´ë¥¼ í¬í•¨
        ocr_related_keywords = ["ì•ŒëŸ¬ì§€", "ì‹ì´ ë²”ìœ„", "ì‹ì´ë²”ìœ„"]
        include_user_info = any(keyword in prompt for keyword in ocr_related_keywords)

        # ì‚¬ìš©ì ì •ë³´ í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ ì§ˆë¬¸ êµ¬ì„±
        question_input = (user_info_text if include_user_info else "") + "ì§ˆë¬¸: " + prompt

        # âœ… ë¬¸ì„œ ê²€ìƒ‰ ë° ìˆ˜ë™ í•„í„°ë§ (FAISSëŠ” í•„í„°ë§ ë¯¸ì§€ì›)
        retrieved_docs = retriever.get_relevant_documents(prompt)
        filtered_docs = [doc for doc in retrieved_docs if doc.metadata.get("source") in document_name]

        # âœ… ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        final_prompt = {
            "question": question_input,
            "ocr_text": st.session_state["ocr_text"],
            "context_docs": "\n\n".join(doc.page_content for doc in filtered_docs)  # ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
        }

        # ë¬¸ì„œê°€ ì—†ê±°ë‚˜ document_nameì´ Noneì´ë©´ ì˜ˆì™¸ ì²˜ë¦¬
        if document_name is None or not filtered_docs:
            unknown_response = "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì€ ì„±ë¶„, ë¹„ê±´, ì•ŒëŸ¬ì§€, í™˜ê²½ ì˜í–¥, ìˆ˜ìì›, ì¹¼ë¡œë¦¬ ë“±ê³¼ ê´€ë ¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."
            chat_message("assistant", unknown_response)
            st.session_state["memory"].chat_memory.add_ai_message(unknown_response)
            st.session_state.messages.append({"role": "assistant", "content": unknown_response})
            st.stop()

        with st.spinner("Thinking..."):
            # ë²¡í„° DB ê¸°ë°˜ ë‹µë³€ ìƒì„± (ì´ë¯¸ RagChainì—ì„œ filtered_docsë¥¼ contextë¡œ ì‚¬ìš© ì¤‘)
            answer_from_db = rag_chain.invoke(final_prompt)

        # âœ… ì‚¬ìš©ì ë¹„ê±´ ì¢…ë¥˜ì— ë”°ë¥¸ ë¶€ì—° ì„¤ëª… ìƒì„± í•¨ìˆ˜
        def get_vegan_type_explanation(user_info):
            types = user_info.get("types", [])
            if not types:
                return "ì‚¬ìš©ìì˜ ë¹„ê±´ ì‹ë‹¨ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶€ì—° ì„¤ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”."

            type_descriptions = {
                "í”„ë£¨í…Œë¦¬ì–¸": "ê³¼ì¼, ì”¨ì•—, ê²¬ê³¼ë¥˜ ì¤‘ì‹¬ì˜ ì‹ë‹¨",
                "ë¹„ê±´": "ëª¨ë“  ë™ë¬¼ì„± ì‹í’ˆì„ ë°°ì œí•˜ëŠ” ì‹ë‹¨",
                "ì˜¤ë³´": "ë‹¬ê±€ì€ í—ˆìš©í•˜ì§€ë§Œ ê¸°íƒ€ ë™ë¬¼ì„± ì‹í’ˆì€ ê¸ˆì§€ë˜ëŠ” ì‹ë‹¨",
                "ë½í† ": "ìš°ìœ ëŠ” í—ˆìš©í•˜ì§€ë§Œ ê¸°íƒ€ ë™ë¬¼ì„± ì‹í’ˆì€ ê¸ˆì§€ë˜ëŠ” ì‹ë‹¨",
                "ë½í† ì˜¤ë³´": "ë‹¬ê±€ê³¼ ìš°ìœ ëŠ” í—ˆìš©ë˜ë©° ê·¸ ì™¸ ë™ë¬¼ì„± ì‹í’ˆì€ ê¸ˆì§€ë˜ëŠ” ì‹ë‹¨",
                "í˜ìŠ¤ì½”": "ìƒì„ ì€ í—ˆìš©ë˜ì§€ë§Œ ìœ¡ë¥˜ì™€ ìœ ì œí’ˆì€ ê¸ˆì§€ë˜ëŠ” ì‹ë‹¨",
                "í”Œë ‰ì‹œí…Œë¦¬ì–¸": "ê°€ë” ë™ë¬¼ì„± ì‹í’ˆì„ ì„­ì·¨í•˜ëŠ” ìœ ì—°í•œ ì‹ë‹¨"
            }

            descriptions = [type_descriptions.get(t, "") for t in types]
            description_text = " / ".join(filter(None, descriptions))

            return f"ì‚¬ìš©ìëŠ” {', '.join(types)} ì‹ë‹¨ì„ ë”°ë¦…ë‹ˆë‹¤. ({description_text}) ì´ ì‹ë‹¨ ê¸°ì¤€ì— ë§ì¶° ë¶€ì—° ì„¤ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”."


        # âœ… GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì´ˆê¸° ì„ì‹œ í”„ë¡¬í”„íŠ¸)
        gpt_prompt = (
            f"ì—¬ê¸° ë²¡í„° DB ê¸°ë°˜ ì„±ë¶„ ë¶„ì„ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤:\n{answer_from_db}\n\n"
            f"{get_vegan_type_explanation(user_info)}"
        )

        # âœ… GPTë¡œ ì§ˆë¬¸ ì£¼ì œ ë¶„ë¥˜
        def classify_question_with_gpt(question: str) -> str:
            classification_prompt = f"""
        ë‹¤ìŒ ì§ˆë¬¸ì˜ ì£¼ì œë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”. í•­ëª©: ë¹„ê±´(v), ì•ŒëŸ¬ì§€(a), ì¹¼ë¡œë¦¬(n), í™˜ê²½ ì˜í–¥(e)
        ì§ˆë¬¸: "{question}"
        ì •ë‹µì€ v, a, n, e ì¤‘ í•˜ë‚˜ë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.
        """
            classification = llm.invoke([{"role": "user", "content": classification_prompt}]).content.strip().lower()
            return classification

        # âœ… ë¶„ë¥˜ëœ ì£¼ì œì— ë”°ë¼ GPT í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±
        question_type = classify_question_with_gpt(prompt)

        if question_type == "v":
            gpt_prompt = (
                f"ë‹¤ìŒì€ ë²¡í„° DB ê¸°ë°˜ ì„±ë¶„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:\n{answer_from_db}\n\n"
                f"{get_vegan_type_explanation(user_info)}"
            )
        elif question_type == "a":
            gpt_prompt = (
                f"ë‹¤ìŒ ì œí’ˆì˜ ì„±ë¶„ ì •ë³´:\n{answer_from_db}\n\n"
                f"ì´ ì œí’ˆì— ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ì´ ìˆëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì„¸ìš”. "
                f"ì£¼ì˜í•´ì•¼ í•  ì•ŒëŸ¬ì§€ ìœ ë°œ ë¬¼ì§ˆì´ ìˆë‹¤ë©´ ëª…í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”."
            )
        elif question_type == "n":
            gpt_prompt = (
                f"ë‹¤ìŒ ì œí’ˆì˜ ì„±ë¶„ ì •ë³´:\n{answer_from_db}\n\n"
                f"ì´ ì œí’ˆì˜ ì¹¼ë¡œë¦¬ ë° ì£¼ìš” ì˜ì–‘ ì •ë³´ë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”. "
                f"í•˜ë£¨ ê¶Œì¥ ì„­ì·¨ëŸ‰ê³¼ ë¹„êµí•´ë„ ì¢‹ì•„ìš”."
            )
        else:
            gpt_prompt = (
                f"ë‹¤ìŒ ì„±ë¶„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì„¤ëª…ì„ ë§ë¶™ì—¬ ì£¼ì„¸ìš”:\n{answer_from_db}"
            )

        # âœ… GPT ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ â†’ ë¶€ì—° ì„¤ëª… ìƒì„±
        response = llm.invoke([{"role": "assistant", "content": gpt_prompt}])
        answer_from_gpt = response.content

        # âœ… ìµœì¢… ì‘ë‹µ ìƒì„± ë° ì¶œë ¥
        combined_answer = (
            f"ë²¡í„° DB ë° OCR ê¸°ë°˜ ë‹µë³€:\n{answer_from_db}\n\n"
            f"GPT ëª¨ë¸ ê¸°ë°˜ ë¶€ì—° ì„¤ëª…:\n{answer_from_gpt}"
        )

        st.session_state.messages.append({"role": "assistant", "content": combined_answer})
        chat_message("assistant", combined_answer)

        # âœ… ì—¬ê¸°ì— ì¡°ê±´ë¶€ ì €ì¥ ì½”ë“œ ì¶”ê°€
        expected_gram = extract_gram_from_prompt(prompt)

        if expected_gram:
            # GPT ê¸°ë°˜ ë‹µë³€ì—ì„œë§Œ ì €ì¥
            store_score_from_response(answer_from_gpt, expected_gram)

        # âœ… ë¬¸ì„œ ê´€ë ¨ ì •ë³´ë„ ì¶œë ¥ (FAISSëŠ” ìˆ˜ë™ í•„í„°ë§ í•„ìš”)
        retrieved_docs = retriever.get_relevant_documents(prompt)
        filtered_docs = [doc for doc in retrieved_docs if doc.metadata.get("source") in document_name]

        if filtered_docs:
            with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                for doc in filtered_docs:
                    source = doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ")
                    st.markdown(f"ğŸ“„ **{source}**", help=doc.page_content)
