# ê·¸ëƒ¥ í™˜ê²½ ì˜í–¥

import os
import streamlit as st
import tempfile
import io
import atexit
import pandas as pd
import re
from google.cloud import vision
from dotenv import load_dotenv
import base64
import difflib
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableMap, RunnableLambda, RunnablePassthrough

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

def show():

    import os
    import streamlit as st
    import json
    from google.cloud import vision
    from google.oauth2 import service_account
    from dotenv import load_dotenv
    
    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ ë¡œì»¬ ê°œë°œìš©)
    load_dotenv()

    if "google" in st.secrets:
        creds_info = json.loads(st.secrets["google"]["credentials"])
        credentials = service_account.Credentials.from_service_account_info(creds_info)
        
    # Streamlit Secretsì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if "google" in st.secrets:
        google_creds = st.secrets["google"]["credentials"]
        creds_dict = json.loads(google_creds)
    
        credentials = service_account.Credentials.from_service_account_info(creds_dict)
        client = vision.ImageAnnotatorClient(credentials=credentials)
    else:
        # ë¡œì»¬ ì‹¤í–‰ ì‹œ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê²½ë¡œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ ìœ ì§€
        credentials_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        client = vision.ImageAnnotatorClient.from_service_account_file(credentials_path)


    def get_image_base64(image_path):
        with open(image_path, "rb") as image_file:
            encoded = base64.b64encode(image_file.read()).decode()
        return f"data:image/png;base64,{encoded}"

    # ë§í’ì„  ìŠ¤íƒ€ì¼ í•¨ìˆ˜ (ì•„ì´ì½˜ ì¶”ê°€)
    def chat_message(role, message):
        if role == "user":
            st.markdown(
                f"""
                <div style="display: flex; justify-content: flex-end; align-items: flex-end; margin: 5px 0;">
                    <div style="background-color: #DCF8C6; padding: 10px 15px; 
                                border-radius: 15px 15px 0px 15px; max-width: 70%;
                                font-size: 16px; word-wrap: break-word;">
                        {message}
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )
        else:
            image_base64 = get_image_base64("ì œëª©.png")
            st.markdown(
                f"""
                <div style="display: flex; justify-content: flex-start; align-items: flex-start; margin: 5px 0;">
                    <img src="{image_base64}" width="45" height="45" style="margin-right: 8px; border-radius: 50%;">
                    <div style="background-color: #FFFFFF; padding: 10px 15px; 
                                border-radius: 15px 15px 15px 0px; max-width: 70%;
                                border: 1px solid #ddd; font-size: 16px; 
                                word-wrap: break-word;">
                        {message}
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )

    # Chroma DBë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì •ì˜
    def close_vectorstore():
        # ì„¸ì…˜ ìƒíƒœì— 'vectorstore'ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if "vectorstore" in st.session_state:
            try:
                # ë‚´ë¶€ Chroma clientë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
                st.session_state["vectorstore"]._client.client.close()
                print("Chroma DB ì•ˆì „ ì¢…ë£Œ ì™„ë£Œ")
            except Exception as e:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë©”ì‹œì§€ ì¶œë ¥
                print(f"Chroma DB ì¢…ë£Œ ì˜¤ë¥˜: {e}")

    # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ close_vectorstore í•¨ìˆ˜ê°€ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ë„ë¡ ë“±ë¡
    atexit.register(close_vectorstore)

    # í´ë¼ì´ì–¸íŠ¸ ìƒì„± í•¨ìˆ˜
    @st.cache_resource
    def get_vision_client():
        if "google" in st.secrets:
            creds_info = json.loads(st.secrets["google"]["credentials"])
            credentials = service_account.Credentials.from_service_account_info(creds_info)
            return vision.ImageAnnotatorClient(credentials=credentials)
        else:
            raise Exception("Google credentials not found in Streamlit secrets.")
    
    # OCR í•¨ìˆ˜ ì •ì˜
    def detect_text(image_path):
        client = get_vision_client()
    
        with io.open(image_path, 'rb') as image_file:
            content = image_file.read()
    
        image = vision.Image(content=content)
        response = client.text_detection(image=image)
    
        if response.error.message:
            raise Exception(f"Google Vision API ì˜¤ë¥˜: {response.error.message}")
    
        texts = response.text_annotations
        if not texts:
            return ""
    
        return texts[0].description

    # ì§ˆë¬¸ ìœ í˜• ë¶„ì„ í•¨ìˆ˜
    def analyze_question_type(prompt):
        if "ì„±ë¶„" in prompt or "ë¶„ì„" in prompt:
            return "ì‹í’ˆí‘œì‹œê¸°ì¤€.pdf"
        elif "ì‹ì´ ë²”ìœ„" in prompt or "ì‹ì´ë²”ìœ„" in prompt or "ë¹„ê±´" in prompt:
            return "ì‹ì´ë²”ìœ„.pdf"
        elif "ì•ŒëŸ¬ì§€" in prompt or "ì•Œë ˆë¥´ê¸°" in prompt:
            return "ì•ŒëŸ¬ì§€.pdf"
        elif "í™˜ê²½ ì˜í–¥" in prompt or "í™˜ê²½ì˜í–¥" in prompt:
            return "AGRIBALYSE.csv"
        elif "ìˆ˜ìì›" in prompt or "ìˆ˜ìì› ì˜í–¥" in prompt:
            return [
                "ìˆ˜ìì›ë¬¸ì„œ.pdf"
            ]
        elif "ì¼ì¼ ì„­ì·¨ëŸ‰" in prompt or "ì¹¼ë¡œë¦¬" in prompt:
            return "ì¹¼ë¡œë¦¬.pdf"
        return None

    # ì‹í’ˆ í•˜ìœ„ ê·¸ë£¹
    FOOD_SUBGROUP_LIST = [
        "ì¡°ë¦¬ ë³´ì¡° ì‹í’ˆ", "í•´ì¡°ë¥˜", "ì¡°ë¯¸ë£Œ", "íŠ¹ë³„í•œ ì‹ì´ë¥¼ ìœ„í•œ ì‹í’ˆ", "í–¥ì‹ ë£Œ", "í—ˆë¸Œ", "ê¸°íƒ€ ì¬ë£Œ", 
        "ì†ŒìŠ¤", "ì†Œê¸ˆë¥˜", "ìœ ì•„ìš© ì‹œë¦¬ì–¼ ë° ë¹„ìŠ¤í‚·", "ìœ ì•„ìš© ì§  ì´ìœ ì‹ ë° ì‹ì‚¬ë¥˜", "í˜ì´ìŠ¤íŠ¸ë¦¬ì™€ ë‹¤ë¥¸ ì• í”¼íƒ€ì´ì €", 
        "í”¼ì, íƒ€ë¥´íŠ¸, ê·¸ë¦¬ê³  í¬ë ˆí˜", "í˜¼í•©ìš”ë¦¬", "ì±„ì‹ ìš”ë¦¬", "í˜¼í•© ìƒëŸ¬ë“œì™€ ìƒì•¼ì±„", "ìƒŒë“œìœ„ì¹˜", "ìˆ˜í”„", 
        "ê¸°íƒ€ ì§€ë°©ë¥˜", "ë²„í„°", "ì–´ìœ  (ìƒì„ ê¸°ë¦„)", "ì‹ë¬¼ì„± ê¸°ë¦„ ë° ì§€ë°©", "ë§ˆê°€ë¦°", "ì•„ì¹¨ìš© ì‹œë¦¬ì–¼ê³¼ ë¹„ìŠ¤í‚·", 
        "ë°€ê°€ë£¨ì™€ ë°˜ì£½", "íŒŒìŠ¤íƒ€, ìŒ€ ë° ê³¡ë¬¼", "ê¸°íƒ€ ìœ¡ë¥˜ ì œí’ˆ", "ê°€ê³µìœ¡", "ìƒë¬¼ê³¼ ê°‘ê°ë¥˜", "ê³„ë€", "ë‚  ìƒì„ ", 
        "ìµíŒ ìƒì„ ", "ìƒì„  ë° í•´ì‚°ë¬¼ ê°€ê³µ ì œí’ˆ", "ìœ¡ê°€ê³µí’ˆ ëŒ€ì²´í’ˆ", "ìœ¡ë¥˜ ëŒ€ì²´í’ˆ", "ìƒê³ ê¸°", "ìµíŒ ê³ ê¸°", 
        "í¬ë¦¼ ë° í¬ë¦¼ ê¸°ë°˜ íŠ¹ìˆ˜ ì‹í’ˆ", "ì¹˜ì¦ˆ", "ìš°ìœ ", "ì‹ ì„ í•œ ìœ ì œí’ˆ ë° ìœ ì‚¬ ì œí’ˆ", "ê³¼ì¼", "ê²¬ê³¼ë¥˜ ë° ìœ ì§€ì¢…ì", 
        "ì±„ì†Œ", "ì½©ë¥˜", "ê°ì ë° ê¸°íƒ€ ë¿Œë¦¬ì±„ì†Œ"
    ]

    # í™˜ê²½ ì˜í–¥ ë²”ì£¼ë³„ ê´€ë ¨ column ì •ì˜
    impact_categories = {
        "ìƒë¬¼í•™ì  ì˜í–¥": [
            "ì¸ê°„ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ë…ì„± ì˜í–¥: ë¹„ì•”ì„± ë¬¼ì§ˆ(ì œí’ˆ ë‹¹ CTUh/kg)",
            "ì¸ê°„ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ë…ì„± ì˜í–¥: ë°œì•” ë¬¼ì§ˆ(ì œí’ˆ ë‹¹ CTUh/kg)",
        ],
        "ëŒ€ê¸° ì˜í–¥": [
            "ê¸°í›„ ë³€í™”(ì œí’ˆ ë‹¹ CO2 eq/kg)",
            "ì˜¤ì¡´ì¸µ íŒŒê´´(ì œí’ˆ ë‹¹ CVC11 eq/kg)",
            "ì´ì˜¨í™” ë°©ì‚¬ì„ (ì œí’ˆ ë‹¹ U-235 eq/kg)",
            "ì˜¤ì¡´ì˜ ê´‘í™”í•™ì  í˜•ì„±(ì œí’ˆ ë‹¹ NMVOC eq/kg)",
            "ë¯¸ì„¸ ë¨¼ì§€(ì œí’ˆ ë‹¹ ì§ˆë³‘ ë°œìƒë¥ /kg)",
        ],
        "í† ì§€ ì˜í–¥": [
            "ìœ¡ìƒ ë° ë‹´ìˆ˜ ì‚°ì„±í™”(ì œí’ˆ ë‹¹ H+ eq/kg)",
            "ìœ¡ìƒ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ N eq/mol)",
            "í† ì§€ ì‚¬ìš©(ì œí’ˆ ë‹¹ Pt/kg)",
            "í™”ì„ ìì› ê³ ê°ˆ(ì œí’ˆ ë‹¹ MJ/kg)",
            "ê´‘ë¬¼ ìì› ê³ ê°ˆ(ì œí’ˆ ë‹¹ Sb eq/kg)",
        ],
        "ìˆ˜ìì› ì˜í–¥" : [
            "ë‹´ìˆ˜ì˜ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ P eq/kg)",
            "í•´ì–‘ì˜ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ N eq/kg)",
            "ë‹´ìˆ˜ ìˆ˜ìƒ ìƒíƒœê³„ì— ëŒ€í•œ ìƒíƒœ ë…ì„±(ì œí’ˆ ë‹¹ CTUe/kg)",
        ]
    }

    impact_factors = {
        "ìœ¡ìƒ ë° ë‹´ìˆ˜ ì‚°ì„±í™”(ì œí’ˆ ë‹¹ H+ eq/kg)": {"nf": 55.56954123, "weight": 6.2},
        "ê¸°í›„ ë³€í™”(ì œí’ˆ ë‹¹ CO2 eq/kg)": {"nf": 7553.083163, "weight": 21.06},
        "ë‹´ìˆ˜ ìˆ˜ìƒ ìƒíƒœê³„ì— ëŒ€í•œ ìƒíƒœ ë…ì„±(ì œí’ˆ ë‹¹ CTUe/kg)": {"nf": 56716.58634, "weight": 1.92},
        "ë‹´ìˆ˜ì˜ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ P eq/kg)": {"nf": 1.606852128, "weight": 2.8},
        "í•´ì–‘ì˜ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ N eq/kg)": {"nf": 19.54518155, "weight": 2.96},
        "ìœ¡ìƒ ë¶€ì˜ì–‘í™”(ì œí’ˆ ë‹¹ N eq/mol)": {"nf": 176.7549998, "weight": 3.71},
        "ì¸ê°„ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ë…ì„± ì˜í–¥: ë°œì•” ë¬¼ì§ˆ(ì œí’ˆ ë‹¹ CTUh/kg)": {"nf": 1.73E-05, "weight": 2.13},
        "ì¸ê°„ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ë…ì„± ì˜í–¥: ë¹„ì•”ì„± ë¬¼ì§ˆ(ì œí’ˆ ë‹¹ CTUh/kg)": {"nf": 0.000129, "weight": 1.84},
        "ì´ì˜¨í™” ë°©ì‚¬ì„ (ì œí’ˆ ë‹¹ U-235 eq/kg)": {"nf": 4220.16339, "weight": 5.01},
        "í† ì§€ ì‚¬ìš©(ì œí’ˆ ë‹¹ Pt/kg)": {"nf": 819498.1829, "weight": 7.94},
        "ì˜¤ì¡´ì¸µ íŒŒê´´(ì œí’ˆ ë‹¹ CVC11 eq/kg)": {"nf": 0.052348383, "weight": 6.31},
        "ë¯¸ì„¸ ë¨¼ì§€(ì œí’ˆ ë‹¹ ì§ˆë³‘ ë°œìƒë¥ /kg)": {"nf": 0.000595, "weight": 8.96},
        "ì˜¤ì¡´ì˜ ê´‘í™”í•™ì  í˜•ì„±(ì œí’ˆ ë‹¹ NMVOC eq/kg)": {"nf": 40.85919773, "weight": 4.78},
        "í™”ì„ ìì› ê³ ê°ˆ(ì œí’ˆ ë‹¹ MJ/kg)": {"nf": 65004.25966, "weight": 8.32},
        "ê´‘ë¬¼ ìì› ê³ ê°ˆ(ì œí’ˆ ë‹¹ Sb eq/kg)": {"nf": 0.063622615, "weight": 7.55},
        "ìˆ˜ìì› ê³ ê°ˆ(ì œí’ˆ ë‹¹ m3 depriv./kg)": {"nf": 11468.70864, "weight": 8.51}
    }


    # ì‚¬ìš©ì ì…ë ¥(prompt_text)ì—ì„œ ì‹í’ˆ í•˜ìœ„ ê·¸ë£¹(FOOD_SUBGROUP_LIST)ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    def match_food_subgroup_from_prompt(prompt_text):
        # FOOD_SUBGROUP_LISTì— ìˆëŠ” ê° ì„œë¸Œê·¸ë£¹ì„ ìˆœíšŒ
        for subgroup in FOOD_SUBGROUP_LIST:
            # ì…ë ¥ í…ìŠ¤íŠ¸ì— í•´ë‹¹ ì„œë¸Œê·¸ë£¹ ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  í™•ì¸
            if subgroup.lower() in prompt_text.lower():
                return subgroup  # ì¼ì¹˜í•˜ëŠ” ì„œë¸Œê·¸ë£¹ì´ ìˆìœ¼ë©´ ë°˜í™˜
        return None  # ì¼ì¹˜í•˜ëŠ” í•­ëª©ì´ ì—†ìœ¼ë©´ None ë°˜í™˜

    # ì˜í–¥ ì¹´í…Œê³ ë¦¬ë³„ ì—´ ì¶”ì¶œ í•¨ìˆ˜
    def extract_impact_columns(prompt):
        # í™˜ê²½ ì˜í–¥ ì¹´í…Œê³ ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í™•ì¸í•˜ì—¬ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì—´ì„ ë°˜í™˜
        for category, columns in impact_categories.items():  # impact_categories ë”•ì…”ë„ˆë¦¬ ìˆœíšŒ
            if category in prompt:  # í˜„ì¬ ì¹´í…Œê³ ë¦¬ëª…ì´ promptì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
                return columns  # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ëŒ€ì‘í•˜ëŠ” ì—´ ëª©ë¡ ë°˜í™˜
        return []  # ì¼ì¹˜í•˜ëŠ” ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì˜¤ë¥˜ ë°©ì§€ìš© ê¸°ë³¸ê°’)


    # í™˜ê²½ ì˜í–¥ ìˆ˜ì¹˜
    def calculate_environmental_impact(prompt, ocr_text):
        # ë²¡í„° DBì—ì„œ "AGRIBALYSE.csv" ë¬¸ì„œë§Œ ê²€ìƒ‰
        vectorstore = st.session_state["vectorstore"]

        # 'í™˜ê²½ ì˜í–¥' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì§ˆë¬¸ë§Œ ì²˜ë¦¬
        document_name = analyze_question_type(prompt)

        # ë””ë²„ê¹…: document_name í™•ì¸
        print(f"ë¬¸ì„œ ì´ë¦„: {document_name}")

        # ë¬¸ì„œ ì´ë¦„ì´ ì—†ìœ¼ë©´, ì¦‰ í‚¤ì›Œë“œ ì¡°ê±´ì— ë¶€í•©í•˜ì§€ ì•Šìœ¼ë©´ ì•ˆë‚´ ë©˜íŠ¸ ì¶œë ¥ í›„ ì¢…ë£Œ
        if not document_name:
            no_match_response = (
                "â—ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
                "ë‹¤ìŒê³¼ ê°™ì€ ì£¼ì œë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”:\n"
                "- ì„±ë¶„ ë¶„ì„, ë¹„ê±´ ì¢…ë¥˜, ì•Œë ˆë¥´ê¸°, í™˜ê²½ ì˜í–¥, ìˆ˜ìì›, ì¹¼ë¡œë¦¬ ë“±"
            )
            chat_message("assistant", no_match_response)
            st.session_state["memory"].chat_memory.add_ai_message(no_match_response)
            st.session_state.messages.append({"role": "assistant", "content": no_match_response})
            return None

        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retriever = vectorstore.as_retriever()
        retriever.search_kwargs = {"filter": {"source": document_name}}
        relevant_docs = retriever.get_relevant_documents(prompt)

        # ë””ë²„ê¹…: ê´€ë ¨ ë¬¸ì„œ ì¶œë ¥
        print(f"ê´€ë ¨ ë¬¸ì„œ: {[doc.metadata.get('product_name') for doc in relevant_docs]}")

        # ì‹í’ˆêµ° í•„í„°ë§ (ì˜ˆ: "ì•„ì¹¨ìš© ì‹œë¦¬ì–¼ê³¼ ë¹„ìŠ¤í‚·")
        subgroup = match_food_subgroup_from_prompt(prompt)
        if subgroup:
            relevant_docs = [doc for doc in relevant_docs if doc.metadata.get('food_subgroup') == subgroup]

        # ë””ë²„ê¹…: ì‹í’ˆêµ° í•„í„°ë§ í›„ ê´€ë ¨ ë¬¸ì„œ ì¶œë ¥
        print(f"ì‹í’ˆêµ° í•„í„°ë§ëœ ë¬¸ì„œ: {[doc.metadata.get('product_name') for doc in relevant_docs]}")

        # OCR í…ìŠ¤íŠ¸ì™€ ì œí’ˆëª… ë¹„êµí•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
        matching_docs = []
        for doc in relevant_docs:
            product_name = doc.metadata.get("product_name", "")
            similarity = difflib.SequenceMatcher(None, ocr_text, product_name).ratio()
            matching_docs.append((similarity, doc))

        # ë””ë²„ê¹…: ìœ ì‚¬ë„ ê³„ì‚°ëœ ë¬¸ì„œ ì¶œë ¥
        print(f"ìœ ì‚¬ë„ ê³„ì‚°ëœ ë¬¸ì„œ: {[(doc[1].metadata.get('product_name'), doc[0]) for doc in matching_docs]}")

        # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ ë¬¸ì„œ ì„ íƒ (ì˜ˆ: ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ í•˜ë‚˜ë§Œ)
        matching_docs.sort(reverse=True, key=lambda x: x[0])
        if not matching_docs:
            print("ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        most_similar_doc = matching_docs[0][1]

        # ë””ë²„ê¹…: ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ í™•ì¸
        print(f"ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ: {most_similar_doc.metadata.get('product_name')}")

        # ì„ íƒëœ í™˜ê²½ ì˜í–¥ í•­ëª© ì¶”ì¶œ
        selected_cols = extract_impact_columns(prompt)

        # ë””ë²„ê¹…: ì„ íƒëœ í™˜ê²½ ì˜í–¥ í•­ëª© ì¶œë ¥
        print(f"ì„ íƒëœ í™˜ê²½ ì˜í–¥ í•­ëª©: {selected_cols}")

        impact_data = []

        if most_similar_doc:
            metadata = most_similar_doc.metadata
            impact_entry = {
                'food_subgroup': metadata.get('food_subgroup'),
                'product_name': metadata.get('product_name')
            }

            for col in selected_cols:
                impact_entry[col] = metadata.get(col, None)

            impact_data.append(impact_entry)

        # ë””ë²„ê¹…: í™˜ê²½ ì˜í–¥ ë°ì´í„° ì¶œë ¥
        print(f"í™˜ê²½ ì˜í–¥ ë°ì´í„°: {impact_data}")

        # ê²°ê³¼ DataFrame ë°˜í™˜
        if impact_data:
            impact_df = pd.DataFrame(impact_data)

            # ë””ë²„ê¹…: ìµœì¢… ê²°ê³¼ DataFrame ì¶œë ¥
            print(f"ê²°ê³¼ DataFrame: {impact_df}")
            return impact_df
        else:
            print("í™˜ê²½ ì˜í–¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

    # ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
    def calculate_score(user_inputs):
        score = 0
        for factor, value in user_inputs.items():
            if factor in impact_factors:  # impact_factorsì—ì„œ í•´ë‹¹ í•­ëª©ì„ ì°¾ìŒ
                weight = impact_factors[factor]["weight"]
                nf = impact_factors[factor]["nf"]
                
                # nfê°€ 0ì´ë©´ ê³„ì‚°ì„ ê±´ë„ˆë›°ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê³„ì‚°
                if nf != 0:
                    factor_score = (value / nf) * weight * 1000  # í•´ë‹¹ factorì— ëŒ€í•œ ì ìˆ˜ ê³„ì‚°
                    score += factor_score
        return score

    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ì—ì„œ ìˆ˜ì¹˜ ê°’ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    def extract_values_from_prompt(prompt):
        print(f"ì…ë ¥ëœ prompt: {prompt}")  # ë””ë²„ê¹…ìš© print
        # ì •ê·œì‹ìœ¼ë¡œ " -"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•­ëª©ê³¼ ê°’ì„ ì¶”ì¶œ
        items_values = re.findall(r"([^\-]+) - ([0-9.E-]+)", prompt)  # " -" ê¸°ì¤€ìœ¼ë¡œ í•­ëª©ê³¼ ê°’ ì¶”ì¶œ
        print(f"ì¶”ì¶œëœ í•­ëª©ê³¼ ê°’: {items_values}")  # ë””ë²„ê¹…ìš© print
        return items_values

    # ì‚¬ìš©ì ì…ë ¥ê°’ì„ ë°›ì„ ìˆ˜ ìˆëŠ” í•¨ìˆ˜ (ë‹¤ìˆ˜ì˜ ì…ë ¥ê°’ì„ ë°›ëŠ” êµ¬ì¡°)
    def get_user_inputs(prompt):
        print(f"ì²˜ë¦¬í•  prompt: {prompt}")  # ë””ë²„ê¹…ìš© print
        items_values = extract_values_from_prompt(prompt)  # ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í•­ëª©ê³¼ ìˆ˜ì¹˜ ê°’ì„ ì¶”ì¶œ
        user_inputs = {}
        
        for item, value in items_values:
            item = item.strip()  # í•­ëª© ì´ë¦„ì˜ ì•ë’¤ ê³µë°± ì œê±°
            value = float(value)  # ê°’ì„ floatë¡œ ë³€í™˜
            user_inputs[item] = value  # í•­ëª©ê³¼ ê°’ì„ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
            print(f"ì…ë ¥ ê°’ ì €ì¥: {item} -> {value}")  # ë””ë²„ê¹…ìš© print
            
        return user_inputs

    # í™˜ê²½ ì˜í–¥ ê³„ì‚° í›„ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
    def calculate_environmental_impact_with_score(prompt):
        print(f"ì ìˆ˜ ê³„ì‚° ìš”ì²­: {prompt}")  # ë””ë²„ê¹…ìš© print
        if "ì ìˆ˜" in prompt or "í™˜ê²½ ì ìˆ˜" in prompt or "í™˜ê²½ ì˜í–¥ ì ìˆ˜" in prompt:  # ì ìˆ˜ ê´€ë ¨ ì§ˆë¬¸ë§Œ ì²˜ë¦¬
            user_inputs = get_user_inputs(prompt)  # ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ê°’ì„ ë°›ìŒ

            if user_inputs:
                # ì ìˆ˜ ê³„ì‚°
                final_score = calculate_score(user_inputs)
                # ì ìˆ˜ ê²°ê³¼ ë°˜í™˜
                score_response = f"ê³„ì‚°ëœ í™˜ê²½ ì˜í–¥ ì ìˆ˜: {final_score:.6f} mPt"
                print(f"ê²°ê³¼ ë°˜í™˜: {score_response}")  # ë””ë²„ê¹…ìš© print
                return score_response
            else:
                return "ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ì…ë ¥ ê°’ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
        return None

    def extract_gram_from_prompt(prompt):
        # ì‚¬ìš©ì ì…ë ¥ì—ì„œ g ë‹¨ìœ„ì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” ìˆ«ìë¥¼ ì¶”ì¶œ (ì˜ˆ: "600g", "150g")
        match = re.search(r'(\d+)\s*g\s*ë‹¹', prompt)
        return int(match.group(1)) if match else None

    def store_score_from_response(response, expected_gram=None):
        if "calorie_scores" not in st.session_state:
            st.session_state["calorie_scores"] = []

        if expected_gram is None:
            print("[âŒ ë¬´ì‹œë¨] g ë‹¨ìœ„ê°€ ì§ˆë¬¸ì— ì—†ìŒ.")
            return

        # ì˜ˆ: '600gë‹¹ 173kcal' ì™€ ì •í™•íˆ ë§¤ì¹­
        pattern = fr'{expected_gram}\s*g\s*ë‹¹.*?(\d+(?:\.\d+)?)\s*(?:kcal|ì¹¼ë¡œë¦¬)'  # 'gë‹¹ ì¹¼ë¡œë¦¬' ì¶”ì¶œ
        match = re.search(pattern, response)
        if match:
            score = float(match.group(1))
            st.session_state["calorie_scores"].append(score)
            print(f"[âœ… ì €ì¥ë¨] {expected_gram}gë‹¹ {score} kcal")
            print(f"[ğŸ“¦ ì „ì²´ ëª©ë¡] {st.session_state['calorie_scores']}")
        else:
            print(f"[âŒ ì €ì¥ ì‹¤íŒ¨] ì‘ë‹µì— {expected_gram}gë‹¹ ì¹¼ë¡œë¦¬ ì •ë³´ê°€ ì—†ìŒ.")


# Streamlit UI ì‹œì‘
# --- ì‚¬ì´ë“œë°”: ì‚¬ìš©ì ì •ë³´ ìš”ì•½ ---
    with st.sidebar:
        st.markdown("### ì‚¬ìš©ì ì •ë³´")

        user_info = st.session_state.get("user_info", {})

        st.markdown(f"**ì´ë¦„:** {user_info.get('name', 'ë¯¸ì…ë ¥')}")
        st.markdown(f"**ë¹„ê±´ ì¢…ë¥˜:** {', '.join(user_info.get('types', [])) if user_info.get('types') else 'ë¯¸ì…ë ¥'}")
        st.markdown(f"**ë‚˜ì´:** {user_info.get('age', 'ë¯¸ì…ë ¥')}")
        st.markdown(f"**ì„±ë³„:** {user_info.get('gender', 'ë¯¸ì…ë ¥')}")
        st.markdown(f"**ì•ŒëŸ¬ì§€:** {user_info.get('allergy', 'ì—†ìŒ')}")

        if st.button("ì •ë³´ ìˆ˜ì •"):
            st.session_state.page = "info"
            st.session_state.from_chatbot = False
            st.rerun()

        uploaded_image = st.file_uploader("ì‹í’ˆ ë¼ë²¨ ì´ë¯¸ì§€ ì—…ë¡œë“œ (png, jpg, jpeg)", type=["png", "jpg", "jpeg"])


    if "memory" not in st.session_state:
        st.session_state["memory"] = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        st.session_state["ocr_text"] = None

    if "calorie_answers" not in st.session_state:  # âœ… ì´ ì¤„ ì¶”ê°€
        st.session_state["calorie_answers"] = []
    
    # âœ… ìƒˆë¡œ ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥
    if uploaded_image is not None:
        # âœ… ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì˜ ì´ë¦„ì´ ì´ì „ê³¼ ë‹¤ë¥¸ ê²½ìš° OCR ì¬ì‹¤í–‰
        if "prev_uploaded_filename" not in st.session_state or st.session_state["prev_uploaded_filename"] != uploaded_image.name:
            st.session_state["prev_uploaded_filename"] = uploaded_image.name  # ìƒˆ ì´ë¯¸ì§€ ì´ë¦„ ì €ì¥

            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                tmp_file.write(uploaded_image.getvalue())
                tmp_path = tmp_file.name

            try:
                ocr_text = detect_text(tmp_path)
                st.session_state["ocr_text"] = ocr_text
                st.session_state["ocr_done"] = True
                st.success("âœ… OCR ì²˜ë¦¬ ì™„ë£Œ! ì¶”ì¶œëœ í…ìŠ¤íŠ¸:")
                st.text_area("OCR í…ìŠ¤íŠ¸", ocr_text, height=300)

                # âœ… ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
                if "vectorstore" not in st.session_state:
                    persist_dir = r"D:\veganism\veganchroma_db"
                    embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")
                    st.session_state["vectorstore"] = Chroma(
                        persist_directory=persist_dir,
                        embedding_function=embedding_function
                    )

                # âœ… ê¸°ì¡´ OCR ë²¡í„° ì‚­ì œ ë° ìƒˆë¡œ ì¶”ê°€
                st.session_state["vectorstore"]._collection.delete(where={"source": "user_ocr"})
                doc = Document(
                    page_content=ocr_text,
                    metadata={"source": "user_ocr", "filename": uploaded_image.name}
                )
                st.session_state["vectorstore"].add_documents([doc])

                system_message = f"ì•„ë˜ëŠ” ì‹í’ˆ ë¼ë²¨ OCR í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤:\n{ocr_text}"
                st.session_state["memory"].chat_memory.add_user_message(system_message)

            except Exception as e:
                st.error(f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.stop()

    # âœ… ìµœì´ˆ ì•ˆë‚´ ë©˜íŠ¸ ì¶œë ¥
    if "messages" not in st.session_state:
        st.session_state.messages = []
        chat_message("assistant", "ì—ì½” ë¹„ê±´ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

    # âœ… ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
    for msg in st.session_state.messages:
        chat_message(msg["role"], msg["content"])

    prompt = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    if prompt:
        # ì‚¬ìš©ì ì •ë³´ êµ¬ì„±
        user_info_text = (
            f"ì‚¬ìš©ì ì •ë³´:\n"
            f"- ì´ë¦„: {user_info.get('name', '')}\n"
            f"- ë¹„ê±´ ì¢…ë¥˜: {', '.join(user_info.get('types', []))}\n"
            f"- ë‚˜ì´: {user_info.get('age', '')}\n"
            f"- ì„±ë³„: {user_info.get('gender', '')}\n"
            f"- ì•ŒëŸ¬ì§€: {user_info.get('allergy', '')}\n\n"
        )

        st.session_state.messages.append({"role": "user", "content": prompt})
        chat_message("user", prompt)

        # âœ… ì¢…ë£Œ ì¸ì‚¬ ê°ì§€ ë° ì‘ë‹µ â†’ ì¦‰ì‹œ return
        farewell_phrases = ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "ì§ˆë¬¸"]
        if any(phrase in prompt.lower() for phrase in farewell_phrases):
            farewell_response = "ì•ˆë…•í•˜ì„¸ìš”. ë¹„ê±°ë‹ˆì¦˜ ì±—ë´‡ì…ë‹ˆë‹¤. ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ë©´ ë­ë“  ë¬¼ì–´ë´ì£¼ì„¸ìš”!"
            chat_message("assistant", farewell_response)
            st.session_state["memory"].chat_memory.add_ai_message(farewell_response)
            st.session_state.messages.append({"role": "assistant", "content": farewell_response})
            return 

        # âœ… ì¢…ë£Œ ì¸ì‚¬ ê°ì§€ ë° ì‘ë‹µ â†’ ì¦‰ì‹œ return
        if any(phrase in prompt for phrase in ["ì´í•©", "ì˜¤ëŠ˜", "í•˜ë£¨", "ì´ ì ìˆ˜", "ì „ì²´ ì ìˆ˜", "ëª¨ë“  ì ìˆ˜"]):
            total_score = sum(st.session_state.get("calorie_scores", []))
            if total_score > 0:
                response = f"ì§€ê¸ˆê¹Œì§€ ì €ì¥ëœ ì¹¼ë¡œë¦¬ ì´í•©ì€ ì•½ {int(total_score)} kcal ì…ë‹ˆë‹¤."
            else:
                response = "ì•„ì§ ì¹¼ë¡œë¦¬ ì •ë³´ë¥¼ ì €ì¥í•œ ì ì´ ì—†ì–´ìš”. ğŸ¤” '(150g ë‹¹ ì¹¼ë¡œë¦¬ëŠ” ëª‡ì´ì•¼?)'ì²˜ëŸ¼ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
            
            chat_message("assistant", response)
            st.session_state["memory"].chat_memory.add_ai_message(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
            return

# ìˆ˜ì • ì¤„ ì‹œì‘        
        keywords = ["ìˆ˜ìì›", "ì„±ë¶„", "ë¹„ê±´", "ì•ŒëŸ¬ì§€", "í™˜ê²½ ì˜í–¥", "ìˆ˜ìì›", "ì¹¼ë¡œë¦¬", "ì‹ì´ë²”ìœ„", "ê°ìì¹©", "í™˜ê²½ ì ìˆ˜", "í™˜ê²½ ì˜í–¥ ì ìˆ˜"]
# ìˆ˜ì • ì¤„  ë


        # 'ì ìˆ˜' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì§ˆë¬¸ì´ë©´ ì ìˆ˜ ê³„ì‚°ë§Œ ì¶œë ¥
        if "ì ìˆ˜" in prompt or "í™˜ê²½ ì ìˆ˜" in prompt or "í™˜ê²½ ì˜í–¥ ì ìˆ˜" in prompt:
            result = calculate_environmental_impact_with_score(prompt)
            
            if result:  # ì ìˆ˜ ê³„ì‚° ê²°ê³¼ê°€ ì¡´ì¬í•˜ë©´
                chat_message("assistant", result)  # ê³„ì‚°ëœ ì ìˆ˜ ì¶œë ¥
                st.session_state["memory"].chat_memory.add_ai_message(result)  # ê³„ì‚°ëœ ê²°ê³¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": result})  # ì„¸ì…˜ ë©”ì‹œì§€ì— ì¶”ê°€
            else:
                # ê³„ì‚° ê°’ì´ ì—†ì„ ê²½ìš°
                error_message = "â— ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ ê°’ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                chat_message("assistant", error_message)  # ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
                st.session_state["memory"].chat_memory.add_ai_message(error_message)  # ì˜¤ë¥˜ ë©”ì‹œì§€ ë©”ëª¨ë¦¬ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": error_message})  # ì˜¤ë¥˜ ë©”ì‹œì§€ ì„¸ì…˜ì— ì¶”ê°€
            return

        # ë‹¤ë¥¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì§ˆë¬¸ì€ ë¹„ê±´ ê´€ë ¨ ë©”ì‹œì§€ ì¶œë ¥
        if not any(keyword in prompt for keyword in keywords):
            no_relevant_msg = (
                "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì€ ë¹„ê±°ë‹ˆì¦˜ì— ê´€ë ¨ëœ ì§ˆë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤.ğŸ˜…ğŸ˜… "
                "ë¹„ê±´ë‹ˆì¦˜ì— ê´€ë ¨ëœ ì§ˆë¬¸ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"
            )
            chat_message("assistant", no_relevant_msg)  # ë‹µë³€ ì¶œë ¥
            st.session_state["memory"].chat_memory.add_ai_message(no_relevant_msg)  # ë©”ëª¨ë¦¬ì— ì €ì¥
            st.session_state.messages.append({"role": "assistant", "content": no_relevant_msg})  # ì„¸ì…˜ ë©”ì‹œì§€ì— ì¶”ê°€
            return


        # âœ… í™˜ê²½ ì˜í–¥ ì§ˆë¬¸ì´ í¬í•¨ëœ ê²½ìš°ì—ë§Œ í™˜ê²½ ì˜í–¥ ê³„ì‚° í•¨ìˆ˜ ì‹¤í–‰
        if "í™˜ê²½ ì˜í–¥" in prompt or "í™˜ê²½ì˜í–¥" in prompt:  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ promptì—ì„œ 'í™˜ê²½ ì˜í–¥' ë˜ëŠ” 'í™˜ê²½ì˜í–¥'ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
            # í™˜ê²½ ì˜í–¥ ê³„ì‚° í•¨ìˆ˜ í˜¸ì¶œ
            impact_df = calculate_environmental_impact(prompt, st.session_state["ocr_text"])  # 'calculate_environmental_impact' í•¨ìˆ˜ í˜¸ì¶œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ OCR í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•˜ì—¬ í™˜ê²½ ì˜í–¥ ë°ì´í„° ê³„ì‚°
            
            if impact_df is not None:  # í™˜ê²½ ì˜í–¥ ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ë°˜í™˜ë˜ì—ˆìœ¼ë©´
                # impact_dfë¥¼ HTML í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥ í˜•ì‹ ì§€ì •
                impact_response = f"<h3>í™˜ê²½ ì˜í–¥ ë°ì´í„°:</h3>{impact_df.to_html(index=False)}"
                chat_message("assistant", impact_response)  # assistant ì—­í• ë¡œ ìƒì„±ëœ í™˜ê²½ ì˜í–¥ ë°ì´í„°ë¥¼ ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ë¡œ ì¶œë ¥
                st.session_state["memory"].chat_memory.add_ai_message(impact_response)  # ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": impact_response})  # ë©”ì‹œì§€ë¥¼ 'assistant' ì—­í• ë¡œ ì„¸ì…˜ ë©”ì‹œì§€ì— ì¶”ê°€

            else:  # ë§Œì•½ í™˜ê²½ ì˜í–¥ ë°ì´í„°ê°€ None(ë¹ˆ ê°’)ì´ë¼ë©´
                chat_message("assistant", "â—ì£„ì†¡í•©ë‹ˆë‹¤. í™˜ê²½ ì˜í–¥ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")  # ë°ì´í„°ê°€ ì—†ë‹¤ëŠ” ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ì¶œë ¥
            return  # í•¨ìˆ˜ ì‹¤í–‰ì„ ì¢…ë£Œí•˜ê³ , ë” ì´ìƒ ë‹¤ë¥¸ ì½”ë“œê°€ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ í•¨
        
        if any(phrase in prompt for phrase in ["ì´í•©", "ì˜¤ëŠ˜", "í•˜ë£¨", "ì´ ì ìˆ˜", "ì „ì²´ ì ìˆ˜", "ëª¨ë“  ì ìˆ˜"]):
            total_score = sum(st.session_state.get("calorie_scores", []))  # âœ… ìˆ˜ì •ëœ ë¶€ë¶„
            if total_score > 0:
                response = f"ì§€ê¸ˆê¹Œì§€ ì €ì¥ëœ ì¹¼ë¡œë¦¬ ì´í•©ì€ ì•½ {int(total_score)} kcal ì…ë‹ˆë‹¤."
            else:
                response = "ì•„ì§ ì¹¼ë¡œë¦¬ ì •ë³´ë¥¼ ì €ì¥í•œ ì ì´ ì—†ì–´ìš”. ğŸ¤” '(150g ë‹¹ ì¹¼ë¡œë¦¬ëŠ” ëª‡ì´ì•¼?)'ì²˜ëŸ¼ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
            
            chat_message("assistant", response)
            st.session_state["memory"].chat_memory.add_ai_message(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

            # í•„ìš” ì‹œ: ì´ ì‘ë‹µì€ ì €ì¥ ì•ˆ í•´ë„ ë¨ (ì´í•© ê²°ê³¼ì´ë¯€ë¡œ)
            return


        if "vectorstore" not in st.session_state or st.session_state["ocr_text"] is None:
            chat_message("assistant", "â— ë¨¼ì € ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° OCR ì²˜ë¦¬ë¥¼ ì™„ë£Œí•´ ì£¼ì„¸ìš”.")
            return

        st.session_state["memory"].chat_memory.add_user_message(prompt)

        llm = ChatOpenAI(temperature=1)
        retriever = st.session_state["vectorstore"].as_retriever()
        document_name = analyze_question_type(prompt)

        if not isinstance(document_name, list):
            document_name = [document_name] if document_name else []
        document_name.append("user_ocr")

        retriever.search_kwargs = {"filter": {"source": {"$in": document_name}}}

        rag_prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ë¹„ê±°ë‹ˆì¦˜ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
        - ì‚¬ìš©ìì˜ ë¹„ê±´ ì¢…ë¥˜ì™€ ì•ŒëŸ¬ì§€ë¥¼ ê³ ë ¤í•´ ìµœëŒ€í•œ ì •í™•íˆ ë‹µí•´ì£¼ì„¸ìš”.

        ì‚¬ìš©ì ì§ˆë¬¸: {question}

        OCR í…ìŠ¤íŠ¸: {ocr_text}

        ì°¸ê³  ë¬¸ì„œ: {context_docs}
        """)

        rag_chain = (
            RunnableMap({
                "question": RunnablePassthrough(),
                "ocr_text": RunnablePassthrough(),
                "context_docs": RunnableLambda(
                    lambda input: (
                        retriever.get_relevant_documents(input["question"])
                        if isinstance(input, dict)
                        else retriever.get_relevant_documents(input)
                    )
                )
                | (lambda docs: "\n\n".join(doc.page_content for doc in docs)),
                "name": RunnablePassthrough(),  # ì¶”ê°€
            })
            | rag_prompt
            | llm
            | StrOutputParser()
        )


        # âœ… OCR ë° ë¹„ê±´ ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œë§Œ ì‚¬ìš©ì ì •ë³´ë¥¼ í¬í•¨
        ocr_related_keywords = ["ì•ŒëŸ¬ì§€", "ì‹ì´ ë²”ìœ„", "ì‹ì´ë²”ìœ„"]
        include_user_info = any(keyword in prompt for keyword in ocr_related_keywords)

        question_input = (user_info_text if include_user_info else "") + "ì§ˆë¬¸: " + prompt

        final_prompt = {
            "name": user_info.get("name", "ì‚¬ìš©ì"),
            "question": question_input,
            "ocr_text": st.session_state["ocr_text"],
            "context_docs": "..."  # ì´ ë¶€ë¶„ì€ retriever í†µí•´ì„œ ë§Œë“  í…ìŠ¤íŠ¸
        }

        docs = retriever.get_relevant_documents(prompt)
        
        if document_name is None:
            unknown_response = "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì€ ì„±ë¶„, ë¹„ê±´, ì•ŒëŸ¬ì§€, í™˜ê²½ ì˜í–¥, ìˆ˜ìì›, ì¹¼ë¡œë¦¬ ë“±ê³¼ ê´€ë ¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."
            chat_message("assistant", unknown_response)
            st.session_state["memory"].chat_memory.add_ai_message(unknown_response)
            st.session_state.messages.append({"role": "assistant", "content": unknown_response})
            return
        
        with st.spinner("Thinking..."):
            # ë²¡í„° DB ê¸°ë°˜ ë‹µë³€ ìƒì„±
            answer_from_db = rag_chain.invoke(final_prompt)

        # âœ… ì‚¬ìš©ì ë¹„ê±´ ì¢…ë¥˜ì— ë”°ë¥¸ ë¶€ì—° ì„¤ëª… í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
        def get_vegan_type_explanation(user_info):
            types = user_info.get("types", [])
            if not types:
                return "ì‚¬ìš©ìì˜ ë¹„ê±´ ì‹ë‹¨ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶€ì—° ì„¤ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”."

            type_descriptions = {
                "í”„ë£¨í…Œë¦¬ì–¸": "ê³¼ì¼, ì”¨ì•—, ê²¬ê³¼ë¥˜ ì¤‘ì‹¬ì˜ ì‹ë‹¨",
                "ë¹„ê±´": "ëª¨ë“  ë™ë¬¼ì„± ì‹í’ˆì„ ë°°ì œí•˜ëŠ” ì‹ë‹¨",
                "ì˜¤ë³´": "ë‹¬ê±€ì€ í—ˆìš©í•˜ì§€ë§Œ ê¸°íƒ€ ë™ë¬¼ì„± ì‹í’ˆì€ ê¸ˆì§€ë˜ëŠ” ì‹ë‹¨",
                "ë½í† ": "ìš°ìœ ëŠ” í—ˆìš©í•˜ì§€ë§Œ ê¸°íƒ€ ë™ë¬¼ì„± ì‹í’ˆì€ ê¸ˆì§€ë˜ëŠ” ì‹ë‹¨",
                "ë½í† ì˜¤ë³´": "ë‹¬ê±€ê³¼ ìš°ìœ ëŠ” í—ˆìš©ë˜ë©° ê·¸ ì™¸ ë™ë¬¼ì„± ì‹í’ˆì€ ê¸ˆì§€ë˜ëŠ” ì‹ë‹¨",
                "í˜ìŠ¤ì½”": "ìƒì„ ì€ í—ˆìš©ë˜ì§€ë§Œ ìœ¡ë¥˜ì™€ ìœ ì œí’ˆì€ ê¸ˆì§€ë˜ëŠ” ì‹ë‹¨",
                "í”Œë ‰ì‹œí…Œë¦¬ì–¸": "ê°€ë” ë™ë¬¼ì„± ì‹í’ˆì„ ì„­ì·¨í•˜ëŠ” ìœ ì—°í•œ ì‹ë‹¨"
            }

            descriptions = [type_descriptions.get(t, "") for t in types]
            description_text = " / ".join(filter(None, descriptions))

            return f"ì‚¬ìš©ìëŠ” {', '.join(types)} ì‹ë‹¨ì„ ë”°ë¦…ë‹ˆë‹¤. ({description_text}) ì´ ì‹ë‹¨ ê¸°ì¤€ì— ë§ì¶° ë¶€ì—° ì„¤ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”."

        # âœ… GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        gpt_prompt = (
            f"ì—¬ê¸° ë²¡í„° DB ê¸°ë°˜ ì„±ë¶„ ë¶„ì„ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤:\n{answer_from_db}\n\n"
            f"{get_vegan_type_explanation(user_info)}"
        )

                # âœ… GPTë¡œ ì§ˆë¬¸ ì£¼ì œ ë¶„ë¥˜
        def classify_question_with_gpt(question: str) -> str:
            classification_prompt = f"""
        ë‹¤ìŒ ì§ˆë¬¸ì˜ ì£¼ì œë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”. í•­ëª©: ë¹„ê±´(v), ì•ŒëŸ¬ì§€(a), ì¹¼ë¡œë¦¬(n), í™˜ê²½ ì˜í–¥(e)
        ì§ˆë¬¸: "{question}"
        ì •ë‹µì€ v, a, n, e ì¤‘ í•˜ë‚˜ë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.
        """
            classification = llm.invoke([{"role": "user", "content": classification_prompt}]).content.strip().lower()
            return classification

        question_type = classify_question_with_gpt(prompt)

        # âœ… GPT í”„ë¡¬í”„íŠ¸ ì£¼ì œë³„ ìƒì„±
        if question_type == "v":
            gpt_prompt = (
                f"ë‹¤ìŒì€ ë²¡í„° DB ê¸°ë°˜ ì„±ë¶„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:\n{answer_from_db}\n\n"
                f"{get_vegan_type_explanation(user_info)}"
            )
        elif question_type == "a":
            gpt_prompt = f"ë‹¤ìŒ ì œí’ˆì˜ ì„±ë¶„ ì •ë³´:\n{answer_from_db}\n\nì´ ì œí’ˆì— ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ì´ ìˆëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì„¸ìš”. ì£¼ì˜í•´ì•¼ í•  ì•ŒëŸ¬ì§€ ìœ ë°œ ë¬¼ì§ˆì´ ìˆë‹¤ë©´ ëª…í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”."
        elif question_type == "n":
            gpt_prompt = f"ë‹¤ìŒ ì œí’ˆì˜ ì„±ë¶„ ì •ë³´:\n{answer_from_db}\n\nì´ ì œí’ˆì˜ ì¹¼ë¡œë¦¬ ë° ì£¼ìš” ì˜ì–‘ ì •ë³´ë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”. í•˜ë£¨ ê¶Œì¥ ì„­ì·¨ëŸ‰ê³¼ ë¹„êµí•´ë„ ì¢‹ì•„ìš”."
        else:
            gpt_prompt = f"ë‹¤ìŒ ì„±ë¶„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì„¤ëª…ì„ ë§ë¶™ì—¬ ì£¼ì„¸ìš”:\n{answer_from_db}"

        # âœ… GPT ë¶€ì—° ì„¤ëª… ìƒì„±
        response = llm.invoke([{"role": "assistant", "content": gpt_prompt}])
        answer_from_gpt = response.content

        # âœ… ìµœì¢… ë‹µë³€ ì¶œë ¥
        combined_answer = (
            f"ë²¡í„° DB ë° OCR ê¸°ë°˜ ë‹µë³€:\n{answer_from_db}\n\n"
            f"GPT ëª¨ë¸ ê¸°ë°˜ ë¶€ì—° ì„¤ëª…:\n{answer_from_gpt}"
        )
        st.session_state.messages.append({"role": "assistant", "content": combined_answer})
        chat_message("assistant", combined_answer)

        
        # âœ… ì—¬ê¸°ì— ì¡°ê±´ë¶€ ì €ì¥ ì½”ë“œ ì¶”ê°€
        expected_gram = extract_gram_from_prompt(prompt)

        if expected_gram:
            # GPT ê¸°ë°˜ ë‹µë³€ì—ì„œë§Œ ì €ì¥
            store_score_from_response(answer_from_gpt, expected_gram)


        # ë¬¸ì„œ ê´€ë ¨ ì •ë³´ë„ ì¶œë ¥
        docs = retriever.get_relevant_documents(prompt)
        if docs:
            with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                for doc in docs:
                    source = doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ")
                    st.markdown(f"ğŸ“„ **{source}**", help=doc.page_content)
